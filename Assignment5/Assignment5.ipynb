{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd077faf-3130-4085-a51d-48807bec42c1",
   "metadata": {},
   "source": [
    "# Assignment 5 - Modeling in Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e37aab",
   "metadata": {},
   "source": [
    "# Creating the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355d2de",
   "metadata": {},
   "source": [
    "Important message is that we tried getting the data from the siibra API, however we encountered many problems along the way. Mainly we were struggling to find any relevant data for all of our regions (V1, V2, V4, IT, MT, LIP). We could only get some info about cell density and structural connectivity for the V1 region in human brain (there is a possiblity that we were fetching the data in a incorrect manner). Nevertheless, in the end we decided to create a synthetic connectivity matrix, based on some biological literature, so that we preserve the neuroanatomical inductive biases of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fb915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HUMAN connectivity:\n",
      "      V1   V2    V4   IT   MT   LIP\n",
      "V1   0.0  0.9  0.00  0.0  0.0  0.00\n",
      "V2   0.0  0.0  0.75  0.0  0.7  0.00\n",
      "V4   0.0  0.0  0.00  0.8  0.0  0.00\n",
      "IT   0.0  0.0  0.00  0.0  0.0  0.00\n",
      "MT   0.0  0.0  0.00  0.0  0.0  0.75\n",
      "LIP  0.0  0.0  0.00  0.0  0.0  0.00\n",
      "\n",
      "MARMOSET connectivity:\n",
      "      V1    V2   V4    IT    MT  LIP\n",
      "V1   0.0  0.85  0.0  0.00  0.00  0.0\n",
      "V2   0.0  0.00  0.7  0.00  0.65  0.0\n",
      "V4   0.0  0.00  0.0  0.75  0.00  0.0\n",
      "IT   0.0  0.00  0.0  0.00  0.00  0.0\n",
      "MT   0.0  0.00  0.0  0.00  0.00  0.7\n",
      "LIP  0.0  0.00  0.0  0.00  0.00  0.0\n",
      "\n",
      "MOUSE connectivity:\n",
      "      V1    V2   V4    IT    MT  LIP\n",
      "V1   0.0  0.75  0.3  0.00  0.35  0.0\n",
      "V2   0.0  0.00  0.6  0.00  0.55  0.0\n",
      "V4   0.0  0.00  0.0  0.65  0.00  0.0\n",
      "IT   0.0  0.00  0.0  0.00  0.00  0.0\n",
      "MT   0.0  0.00  0.0  0.00  0.00  0.6\n",
      "LIP  0.0  0.00  0.0  0.00  0.00  0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# we found that there are 6 visual areas common across the three species\n",
    "# V1, V2, V4, IT, MT, LIP\n",
    "\n",
    "# for the ventral stream: V1 -> V2 -> V4 -> IT\n",
    "# for the dorsal stream: V1 -> V2 -> MT -> LIP\n",
    "# so the V1,V2 are shared, then the streams split into two paths\n",
    "\n",
    "regions = ['V1', 'V2', 'V4', 'IT', 'MT', 'LIP']\n",
    "\n",
    "# we assumed the connection strengths between the above mentioned regions based on the following studies:\n",
    "\n",
    "# Felleman, D. J., & Van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. Cerebral cortex (New York, N.Y. : 1991), 1(1), 1–47. https://doi.org/10.1093/cercor/1.1.1-a\n",
    "# Kaas, J. H. (2001). The organization of sensory cortex. Current Opinion in Neurobiology, 11(4), 498–504. https://doi.org/10.1016/S0959-4388(00)00240-3\n",
    "\n",
    "def get_connectivity_matrix(species):\n",
    "    \"\"\"\n",
    "    Create connectivity matrix for visual pathways.\n",
    "    \"\"\"\n",
    "    connectivity = np.zeros((6, 6))\n",
    "    \n",
    "    if species == 'human':    \n",
    "        connectivity[0, 1] = 0.90 # connection between V1 and V2 (ventral and dorsal stream shared)\n",
    "        connectivity[1, 2] = 0.75 # connection between V2 and V4 (ventral stream)\n",
    "        connectivity[2, 3] = 0.80 # connection between V4 and IT (ventral stream)\n",
    "        connectivity[1, 4] = 0.70 # connection between V2 and MT (dorsal stream)\n",
    "        connectivity[4, 5] = 0.75 # connection between MT and LIP (dorsal stream)\n",
    "        \n",
    "    elif species == 'marmoset':\n",
    "        connectivity[0, 1] = 0.85 # V1→V2\n",
    "        connectivity[1, 2] = 0.70 # V2→V4\n",
    "        connectivity[2, 3] = 0.75 # V4→IT\n",
    "        connectivity[1, 4] = 0.65 # V2→MT\n",
    "        connectivity[4, 5] = 0.70 # MT→LIP\n",
    "        \n",
    "    elif species == 'mouse':\n",
    "        connectivity[0, 1] = 0.75 # V1→V2\n",
    "        connectivity[1, 2] = 0.60 # V2→V4\n",
    "        connectivity[2, 3] = 0.65 # V4→IT\n",
    "        connectivity[1, 4] = 0.55 # V2→MT\n",
    "        connectivity[4, 5] = 0.60 # MT→LIP\n",
    "        \n",
    "        # we \n",
    "        connectivity[0, 2] = 0.30  # V1→V4 shortcut\n",
    "        connectivity[0, 4] = 0.35  # V1→MT shortcut\n",
    "    \n",
    "    return connectivity\n",
    "\n",
    "# generate data for each species\n",
    "species_data = {}\n",
    "for species in ['human', 'marmoset', 'mouse']:\n",
    "    species_data[species] = get_connectivity_matrix(species)\n",
    "    print(f\"\\n{species} connectivity:\")\n",
    "    df = pd.DataFrame(species_data[species], index=regions, columns=regions)\n",
    "    print(df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded33ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    species  V1_to_V2  V2_to_V4  V4_to_IT  V2_to_MT  MT_to_LIP\n",
      "0     human      0.90      0.75      0.80      0.70       0.75\n",
      "1  marmoset      0.85      0.70      0.75      0.65       0.70\n",
      "2     mouse      0.75      0.60      0.65      0.55       0.60\n"
     ]
    }
   ],
   "source": [
    "pathway_data = [] # list to hold pathway strengths for each species in a format suitable for a DataFrame\n",
    "\n",
    "for species in ['human', 'marmoset', 'mouse']:\n",
    "    conn = species_data[species]\n",
    "    # Region indices: V1=0, V2=1, V4=2, IT=3, MT=4, LIP=5    \n",
    "    pathway_data.append({\n",
    "        'species': species,\n",
    "        'V1_to_V2': conn[0, 1],\n",
    "        'V2_to_V4': conn[1, 2],\n",
    "        'V4_to_IT': conn[2, 3],\n",
    "        'V2_to_MT': conn[1, 4],\n",
    "        'MT_to_LIP': conn[4, 5],\n",
    "    })\n",
    "\n",
    "pathway_df = pd.DataFrame(pathway_data)\n",
    "print(pathway_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7d0ce",
   "metadata": {},
   "source": [
    "# CNN model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a28b4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpeciesVisualCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, species_name, connectivity):\n",
    "        super().__init__()\n",
    "        self.species = species_name\n",
    "        \n",
    "        # weights between areas are based on the connectivity strength between regions (from previous cell)\n",
    "        self.w_v1_v2 = connectivity[0, 1]\n",
    "        self.w_v2_v4 = connectivity[1, 2] \n",
    "        self.w_v4_it = connectivity[2, 3]\n",
    "        self.w_v2_mt = connectivity[1, 4]\n",
    "        self.w_mt_lip = connectivity[4, 5]\n",
    "        \n",
    "         # the model shares two early convolutional layers (V1 and V2) - adapted for FashionMNIST dataset (1 input channel, 28x28 images)\n",
    "        self.v1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # 28x28 -> 28x28\n",
    "        self.v2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # 28x28 -> 28x28\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Will use after each layer\n",
    "        \n",
    "        # then it splits into two parallel paths:\n",
    "        # ventral stream, namely V4 and IT \n",
    "        self.v4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.it = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "         \n",
    "        # at the end we have a fully connected layer for classification\n",
    "        self.fc_ventral = nn.Linear(256 * 3 * 3, 10)  # 10 FashionMNIST classes, 256 is the number of features in the last layer and 3*3 is the size after all pooling\n",
    "        \n",
    "        # dorsal stream - MT and LIP (we include it however we do not use it for FashionMNIST, as it is not spatial task)\n",
    "        self.mt = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.lip = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "         \n",
    "        # at the end we have a fully connected layer for regression\n",
    "        self.fc_dorsal = nn.Linear(256 * 3 * 3, 2)  # the output is just (x, y) center position, 256 is the number of features in the last layer and 3*3 is the size after pooling\n",
    "        \n",
    "    def forward(self, x):\n",
    "        v1_out = torch.relu(self.v1(x))  \n",
    "        v1_out = self.pool(v1_out) * self.w_v1_v2   #reduce spatial dimensions to 14x14\n",
    "        \n",
    "        v2_out = torch.relu(self.v2(v1_out))  # weight by V1->V2 strength\n",
    "        v2_out = self.pool(v2_out)   # reduce spatial dimensions to 7x7\n",
    "        \n",
    "        v4_out = torch.relu(self.v4(v2_out)) * self.w_v2_v4  # weight by V2->V4 strength\n",
    "        it_out = torch.relu(self.it(v4_out)) * self.w_v4_it  # weight by V4->IT strength\n",
    "        it_out = self.pool(it_out)  # 3x3\n",
    "        it_flat = it_out.view(it_out.size(0), -1)  # flatten\n",
    "        \n",
    "        object_class = self.fc_ventral(it_flat) # final classification output\n",
    "\n",
    "        mt_out = torch.relu(self.mt(v2_out)) * self.w_v2_mt # weight by V2->MT strength\n",
    "        lip_out = torch.relu(self.lip(mt_out)) * self.w_mt_lip # weight by MT->LIP strength\n",
    "        lip_out = self.pool(lip_out)  # 3x3 \n",
    "        lip_flat = lip_out.view(lip_out.size(0), -1) # Flatten\n",
    "        \n",
    "        spatial_info = self.fc_dorsal(lip_flat) # final regression output\n",
    "        \n",
    "        return object_class, spatial_info # return both outputs so we can know what is on the image (the ventral stream) and where it is located (the dorsal stream)\n",
    "    \n",
    "# we create a model for each species\n",
    "models = {}\n",
    "for species in ['human', 'marmoset', 'mouse']:\n",
    "    models[species] = SpeciesVisualCNN(species, species_data[species])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c4990",
   "metadata": {},
   "source": [
    "# Training the model on the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a06486",
   "metadata": {},
   "source": [
    "## Data preprocessing and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05af6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets created:\n",
      "  Train: 60000 samples\n",
      "  Test: 10000 samples\n",
      "torch.Size([1, 28, 28])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# now we need to train the model so it can actually learn to recognize objects and their locations\n",
    "# for that we will use the FashionMNIST dataset from Kaggle (https://www.kaggle.com/datasets/zalando-research/fashionmnist?resource=download)\n",
    "# first we need to prepare the data, particularly we need to create a PyTorch Dataset and DataLoader for model training. Additionally, we need to reshape the images to 28x28 (because the FashionMNIST images are 28x28) and normalize them to [0, 1] (for more stable training)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "train_df = pd.read_csv('Fashion_MNIST/fashion-mnist_train.csv')\n",
    "test_df = pd.read_csv('Fashion_MNIST/fashion-mnist_test.csv')\n",
    "\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['label'].values\n",
    "        self.pixels = df.drop('label', axis=1).values  # 784 pixel values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get pixels and reshape to 28x28 image (because FashionMNIST images are 28x28 and the original dataset is flattened meaning 28*28 = 784 pixels)\n",
    "        pixels = self.pixels[idx].reshape(28, 28)\n",
    "        # normalize to [0, 1] so that pixel values are between 0 and 1 - it allows for more stable training\n",
    "        pixels = pixels / 255.0 \n",
    "        # add channel dimension: (1, 28, 28) for grayscale (the images in the dataset are grayscale), because PyTorch expects the input shape to be (batch_size, channels, height, width)\n",
    "        image = torch.FloatTensor(pixels).unsqueeze(0)\n",
    "        label = torch.LongTensor([self.labels[idx]])[0]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_dataset = FashionMNISTDataset(train_df)\n",
    "test_dataset = FashionMNISTDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # shuffle for training\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False) \n",
    "\n",
    "print(f\"\\nDatasets created:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")\n",
    "\n",
    "print(train_dataset[0][0].shape)  # the image tensor (1, 28, 28)\n",
    "print(train_dataset[0][1].item()) # the label (0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4740c07b",
   "metadata": {},
   "source": [
    "## Actual training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6313c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the human model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 938/938 [09:05<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.3088, Train Acc=88.76%, Test Loss=0.2805, Test Acc=89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 938/938 [08:48<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.2547, Train Acc=90.75%, Test Loss=0.2373, Test Acc=91.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 938/938 [08:50<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.2243, Train Acc=91.75%, Test Loss=0.2169, Test Acc=91.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 938/938 [09:30<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.1972, Train Acc=92.73%, Test Loss=0.2116, Test Acc=92.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 938/938 [09:02<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.1732, Train Acc=93.59%, Test Loss=0.2062, Test Acc=92.50%\n",
      "Training the marmoset model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 938/938 [09:07<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.5136, Train Acc=81.19%, Test Loss=0.3336, Test Acc=88.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 938/938 [08:44<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.3101, Train Acc=88.70%, Test Loss=0.2607, Test Acc=90.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 938/938 [09:26<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.2628, Train Acc=90.51%, Test Loss=0.2417, Test Acc=90.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 938/938 [09:08<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.2309, Train Acc=91.50%, Test Loss=0.2434, Test Acc=90.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 938/938 [09:26<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.2065, Train Acc=92.41%, Test Loss=0.2244, Test Acc=91.52%\n",
      "Training the mouse model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 938/938 [09:20<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.5065, Train Acc=81.39%, Test Loss=0.3511, Test Acc=87.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 938/938 [07:55<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.3097, Train Acc=88.72%, Test Loss=0.2704, Test Acc=89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 938/938 [09:22<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.2647, Train Acc=90.44%, Test Loss=0.2394, Test Acc=91.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 938/938 [09:43<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.2366, Train Acc=91.42%, Test Loss=0.2334, Test Acc=91.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 938/938 [08:15<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.2106, Train Acc=92.28%, Test Loss=0.2194, Test Acc=91.94%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=10, learning_rate=0.001):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # cross-entropy loss for multi-class classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer\n",
    "    \n",
    "    # dictionary to track metrics\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'test_loss': [], 'test_acc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train() # set model to training mode\n",
    "        train_loss = 0.0 # cumulative loss\n",
    "        correct = 0 # number of correct predictions\n",
    "        total = 0 # total number of samples\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'): # iterate over batches\n",
    "            images, labels = images.to(device), labels.to(device) # move to device\n",
    "            \n",
    "            # Forward pass\n",
    "            obj_out, spatial_out = model(images)  # the output of the ventral stream (object classification), but we ignore spatial_out \n",
    "            loss = criterion(obj_out, labels) # compute loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad() # reset the gradients to zero before backpropagation so that they do not accumulate \n",
    "            loss.backward() # backpropagate the loss\n",
    "            optimizer.step() # update the weights\n",
    "            \n",
    "            train_loss += loss.item() # accumulate loss\n",
    "            _, predicted = obj_out.max(1) # get the index of the max log-probability of each sample\n",
    "            correct += (predicted == labels).sum().item() # count correct predictions\n",
    "            total += labels.size(0) # accumulate total samples\n",
    "        \n",
    "        train_loss /= len(train_loader) # average loss over batches\n",
    "        train_acc = 100.0 * correct / total # training accuracy\n",
    "        \n",
    "        model.eval() # set model to evaluation mode\n",
    "        test_loss = 0.0 # cumulative test lossx \n",
    "        correct = 0 # number of correct predictions\n",
    "        total = 0 # total number of samples\n",
    "        \n",
    "        with torch.no_grad(): # we dont need to compute gradients during evaluation\n",
    "            for images, labels in test_loader: # we iterate over test batches\n",
    "                images, labels = images.to(device), labels.to(device) # move to device\n",
    "                obj_out, _ = model(images) # forward pass, ignore spatial output\n",
    "                loss = criterion(obj_out, labels) # compute loss\n",
    "                 \n",
    "                test_loss += loss.item() # accumulate loss\n",
    "                _, predicted = obj_out.max(1) # get the index of the max log-probability\n",
    "                correct += (predicted == labels).sum().item() # count correct predictions\n",
    "                total += labels.size(0) # accumulate total samples\n",
    "        \n",
    "        test_loss /= len(test_loader) # average loss over batches\n",
    "        test_acc = 100.0 * correct / total # test accuracy\n",
    "        \n",
    "        # we append to the history dictionary\n",
    "        history['train_loss'].append(train_loss) \n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, 'f'Test Loss={test_loss:.4f}, Test Acc={test_acc:.2f}%') \n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "results = {}\n",
    "for species in ['human', 'marmoset', 'mouse']: # train each species model with a learning rate 0.001\n",
    "    print(f\"Training the {species} model\")\n",
    "    model = models[species]\n",
    "    history = train_model(model, train_loader, test_loader, epochs=5, learning_rate=0.001)\n",
    "    results[species] = history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
